# Model selection

> *All models are wrong; some are useful.*

Model selection means choosing the "best" model among a set of competing models. There are several dimensions that we might want to consider; among others:

* Sensitivity: we want all the important parameters.
* Specificity, parsimony: we favor simpler models, with fewer parameters.
* Future predictive ability
* Selection consistency

Usually it is a balance between goodness of fit (how well the model describes the data) and parsimony (to avoid over-fitting). It can be interpreted as the trade-off between bias and variance.

## Cross-validation

We split the data in two parts. We train the model using one of them (the "training data"), and test it on the other (the "testing data").

## Information approaches

We want to minimize an information criterion *GIC*:

$GIC=\lambda|\theta|-2\hat{L}(\theta,D)$

Where:

* $\lambda$ is a factor controlling the penalty for complexity.
* $\hat{L}(\theta,D)$ is the likelihood function of the model.

### Likelihood

The likelihood of a model M is $\hat{L}=p(x|\hat{\theta},M)$. It assumes the data is generated by the model plus some gaussian noise.

### Akaike Information Criterion

In Akaike Information Criterion (AIC) $\lambda=2$.

### Bayesian Information Criterion

In Bayesian Information Criterion (BIC) $\lambda=log(n)$, where n is the number of observations in the data.

## Adaptative model selections

The $\lambda$ factor is a data-adaptive penalty derived using the generalized degrees of freedom for a given modeling procedure.

## Bayesian approaches

Google: Bayes factor, model evidence.

## Sources

* [Some random master thesis is a decent introduction. ](http://scholarworks.gsu.edu/cgi/viewcontent.cgi?article=1100&context=math_theses)
